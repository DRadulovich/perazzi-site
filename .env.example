# Public-facing URLs (no secrets)
NEXT_PUBLIC_SITE_URL=https://perazzi.example.com
NEXT_PUBLIC_SANITY_PREVIEW_ORIGIN=http://localhost:3000

# Sanity configuration (IDs & versions are not secrets, but tokens are)
SANITY_PROJECT_ID=perazzi-project-id
SANITY_DATASET=production
SANITY_API_VERSION=2025-01-01

NEXT_PUBLIC_SANITY_PROJECT_ID=perazzi-project-id
NEXT_PUBLIC_SANITY_DATASET=production
NEXT_PUBLIC_SANITY_API_VERSION=2025-01-01

# Sanity tokens – REPLACE with real tokens in your local .env files
SANITY_WRITE_TOKEN=WRITE_TOKEN_REPLACE_ME
SANITY_READ_TOKEN=READ_TOKEN_REPLACE_ME

# DEV ONLY - do not use in production
NEXT_PUBLIC_SANITY_BROWSER_TOKEN=DO_NOT_USE_IN_PROD_REPLACE_ME

# Cloudinary – account + API credentials
CLOUDINARY_CLOUD_NAME=your-cloud-name
CLOUDINARY_API_KEY=CLOUDINARY_KEY_REPLACE_ME
CLOUDINARY_API_SECRET=CLOUDINARY_SECRET_REPLACE_ME

# Analytics – public write key (still keep real value out of git)
NEXT_PUBLIC_ANALYTICS_WRITE_KEY=ANALYTICS_WRITE_KEY_REPLACE_ME

# Cal.com platform (needed for the booking scheduler)
NEXT_PUBLIC_CAL_OAUTH_CLIENT_ID=CAL_CLIENT_ID_REPLACE_ME
NEXT_PUBLIC_CAL_API_URL=https://api.cal.com/v2

# === AI / OpenAI / PerazziGPT configuration ===
# Used for embeddings + Concierge API and related AI features.
#
# Local dev:
#   - Set OPENAI_API_KEY.
#   - Leave AI_GATEWAY_URL and AI_GATEWAY_TOKEN empty.
#
# Production (Vercel):
#   - Configure AI Gateway with your OpenAI key.
#   - Set AI_GATEWAY_URL and AI_GATEWAY_TOKEN from Vercel AI Gateway.
#   - Do NOT set OPENAI_API_KEY directly in Vercel for production unless you intentionally want to allow a direct-bypass fallback.

# Direct OpenAI key (used in local dev or when Gateway is not configured)
OPENAI_API_KEY=sk-REPLACE_ME

# Vercel AI Gateway endpoint and token (used in production/remote environments)
AI_GATEWAY_URL=
AI_GATEWAY_TOKEN=

# Optional override: "true" to force direct OpenAI via OPENAI_API_KEY even if AI_GATEWAY_* are set (useful for local/CLI)
AI_FORCE_DIRECT=

# Conversation state strategy:
# - "thread" uses Responses API previous_response_id for multi-turn state.
# - When set to "thread", the server will ignore any assistant-history items sent by the client and only forward the latest user message as Responses input.
PERAZZI_CONVO_STRATEGY=thread

# OpenAI Responses API "store" flag (persists responses for later retrieval in the OpenAI dashboard).
PERAZZI_OPENAI_STORE=true

# Debug logging for AI prompts and OpenAI request/response summaries (verification / local dev).
# When true, the server will log input counts, previous_response_id presence, and store flags.
PERAZZI_DEBUG_PROMPT=true

# Admin-only debug overlay (response-only; never shown to normal users).
# Requires BOTH PERAZZI_ADMIN_DEBUG=true and a matching x-perazzi-admin-debug header token.
PERAZZI_ADMIN_DEBUG=true
PERAZZI_ADMIN_DEBUG_TOKEN=REPLACE_WITH_LONG_RANDOM_STRING
PERAZZI_SAFE_DISPLAY_TITLES=true              # "true" to sanitize titles in admin debug (avoid PII/leakage)


# PERAZZI_ALLOW_CHAT_COMPLETIONS=false  # Legacy only. Set true to enable deprecated runChatCompletion()

# Perazzi-specific model and behavior settings (Responses-first; completions env kept for back-compat)
PERAZZI_MODEL=gpt-5.2                         # Preferred model for Responses; set gpt-5.2-pro for max quality (Responses-only; slower/costlier)
PERAZZI_RESPONSES_MODEL=                      # Optional alias; overrides PERAZZI_MODEL if set
PERAZZI_COMPLETIONS_MODEL=                    # Deprecated; still read as a fallback if the above are empty
PERAZZI_MAX_OUTPUT_TOKENS=5000                # Max output tokens (includes visible + reasoning tokens)
PERAZZI_MAX_COMPLETION_TOKENS=                # Deprecated fallback for max tokens
PERAZZI_REASONING_EFFORT=medium                     # none|low|medium|high|xhigh (GPT-5 family; "none" = minimal reasoning)
PERAZZI_TEXT_VERBOSITY=                       # low|medium|high
PERAZZI_PROMPT_CACHE_RETENTION=24h               # in_memory|24h (prompt caching; underscore form only)
PERAZZI_PROMPT_CACHE_KEY=                     # Optional cache key for prompt caching (now wired through)
PERAZZI_EMBED_MODEL=text-embedding-3-large    # Embedding model for retrieval
PERAZZI_RETRIEVAL_LIMIT=15                    # How many chunks to fetch from vector search
PERAZZI_RETRIEVAL_EXCERPT_CHARS=1000          # Max chars per retrieved excerpt inserted into the model prompt
PERAZZI_RETRIEVAL_TOTAL_CHARS=8000            # Max total chars for the full retrieved references prompt block
PERAZZI_RETRIEVAL_POLICY=hybrid               # hybrid|always (hybrid = default retrieve, skip only for clearly meta/UI prompts; always = rollback)
PERAZZI_LOW_CONF_THRESHOLD=                   # Threshold for low-confidence responses
PERAZZI_REQUIRE_GENERAL_LABEL=true            # Defaults true; set false|0|no|off to disable the "General answer (not sourced from Perazzi docs): ..." first-line label in general/unsourced mode
PERAZZI_POST_VALIDATE_OUTPUT=false            # Backstop output validator: "true" enables strict post-generation blocking + label/qualifier enforcement; default false for one-switch rollback
PERAZZI_MODELS_REGISTRY_SOT=true              # Prefer model/spec facts from V2_RAG_corpus-models-details.json (default true in dev)
PERAZZI_ENABLE_FILE_LOG=                      # "true" to write conversation logs locally
EMBED_BATCH_SIZE=64                           # Batch size for embedding ingest scripts

# Logging is OFF by default in this template. Enable to populate PGPT Insights; recommended for local/staging only.
PERAZZI_AI_LOGGING_ENABLED=false
PGPT_INSIGHTS_ALLOW_PROD=true

# Text logging truncates stored prompts/responses by default; avoid "full" except short, consented debugging.
PERAZZI_LOG_TEXT_MODE=truncate                # omitted|truncate|full

# Max chars to persist for prompt/response when truncating (keeps default leakage bounded).
PERAZZI_LOG_TEXT_MAX_CHARS=8000              # Max chars to persist for prompt/response
PERAZZI_ASSISTANT_TEMPERATURE=1.0            # Default temperature for Perazzi Assistant route (only when reasoning=none)
PERAZZI_SOUL_JOURNEY_TEMPERATURE=0.6         # Default temperature for Soul Journey route (only when reasoning=none)

# === ZR1 (Archetype Analysis) feature flags ===
# Keep these OFF in production until ZR1 is validated.
PERAZZI_ENABLE_RERANK=false
PERAZZI_RERANK_CANDIDATE_LIMIT=200
PERAZZI_ARCHETYPE_CONFIDENCE_MIN=0.08
PERAZZI_ENABLE_RETRIEVAL_DEBUG=false

# --- Archetype tuning (safe defaults) ---
# Strength of archetype bias boost in retrieval (0 = off, typical 0.05-0.15)
PERAZZI_ARCHETYPE_BOOST_K=0.08

# Smoothing factor for incremental vector updates
# 1.0  = keep previous vector, 0.0 = replace entirely with new message
PERAZZI_SMOOTHING_FACTOR=0.75

# Postgres / pgvector for the knowledge base
DATABASE_URL=postgres://user:password@localhost:5433/perazzi
PGVECTOR_DIM=3072          # match your embedding model's dimension
PGSSL_MODE=disable         # "require" in prod, "disable" for local dev
