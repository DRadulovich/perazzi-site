# Public-facing URLs (no secrets)
NEXT_PUBLIC_SITE_URL=https://perazzi.example.com
NEXT_PUBLIC_SANITY_PREVIEW_ORIGIN=http://localhost:3000

# Sanity configuration (IDs & versions are not secrets, but tokens are)
SANITY_PROJECT_ID=perazzi-project-id
SANITY_DATASET=production
SANITY_API_VERSION=2025-01-01

NEXT_PUBLIC_SANITY_PROJECT_ID=perazzi-project-id
NEXT_PUBLIC_SANITY_DATASET=production
NEXT_PUBLIC_SANITY_API_VERSION=2025-01-01

# Sanity tokens – REPLACE with real tokens in your local .env files
SANITY_WRITE_TOKEN=WRITE_TOKEN_REPLACE_ME
SANITY_READ_TOKEN=READ_TOKEN_REPLACE_ME

# DEV ONLY - do not use in production
NEXT_PUBLIC_SANITY_BROWSER_TOKEN=DO_NOT_USE_IN_PROD_REPLACE_ME

# Cloudinary – account + API credentials
CLOUDINARY_CLOUD_NAME=your-cloud-name
CLOUDINARY_API_KEY=CLOUDINARY_KEY_REPLACE_ME
CLOUDINARY_API_SECRET=CLOUDINARY_SECRET_REPLACE_ME

# Analytics – public write key (still keep real value out of git)
NEXT_PUBLIC_ANALYTICS_WRITE_KEY=ANALYTICS_WRITE_KEY_REPLACE_ME

# Cal.com platform (needed for the booking scheduler)
NEXT_PUBLIC_CAL_OAUTH_CLIENT_ID=CAL_CLIENT_ID_REPLACE_ME
NEXT_PUBLIC_CAL_API_URL=https://api.cal.com/v2

# === AI / OpenAI / PerazziGPT configuration ===
# Used for embeddings + Concierge API and related AI features.
#
# Local dev:
#   - Set OPENAI_API_KEY.
#   - Leave AI_GATEWAY_URL and AI_GATEWAY_TOKEN empty.
#
# Production (Vercel):
#   - Configure AI Gateway with your OpenAI key.
#   - Set AI_GATEWAY_URL and AI_GATEWAY_TOKEN from Vercel AI Gateway.
#   - Do NOT set OPENAI_API_KEY directly in Vercel for production unless you intentionally want to allow a direct-bypass fallback.

# Direct OpenAI key (used in local dev or when Gateway is not configured)
OPENAI_API_KEY=sk-REPLACE_ME

# Vercel AI Gateway endpoint and token (used in production/remote environments)
AI_GATEWAY_URL=
AI_GATEWAY_TOKEN=

# Optional override: "true" to force direct OpenAI via OPENAI_API_KEY even if AI_GATEWAY_* are set (useful for local/CLI)
AI_FORCE_DIRECT=

# Perazzi-specific model and behavior settings (Responses-first; completions env kept for back-compat)
PERAZZI_MODEL=gpt-5.2                         # Preferred model for Responses; set gpt-5.2-pro for max quality (Responses-only; slower/costlier)
PERAZZI_RESPONSES_MODEL=                      # Optional alias; overrides PERAZZI_MODEL if set
PERAZZI_COMPLETIONS_MODEL=                    # Deprecated; still read as a fallback if the above are empty
PERAZZI_MAX_OUTPUT_TOKENS=5000                # Max output tokens (includes visible + reasoning tokens)
PERAZZI_MAX_COMPLETION_TOKENS=                # Deprecated fallback for max tokens
PERAZZI_REASONING_EFFORT=                     # none|low|medium|high|xhigh (GPT-5 family; "none" = minimal reasoning)
PERAZZI_TEXT_VERBOSITY=                       # low|medium|high
PERAZZI_PROMPT_CACHE_RETENTION=24h               # in_memory|24h (prompt caching; underscore form only)
PERAZZI_PROMPT_CACHE_KEY=                     # Optional cache key for prompt caching (now wired through)
PERAZZI_EMBED_MODEL=text-embedding-3-large    # Embedding model for retrieval
PERAZZI_RETRIEVAL_LIMIT=15                    # How many chunks to fetch from vector search
PERAZZI_LOW_CONF_THRESHOLD=                   # Threshold for low-confidence responses
PERAZZI_ENABLE_FILE_LOG=                      # "true" to write conversation logs locally
EMBED_BATCH_SIZE=64                           # Batch size for embedding ingest scripts

# Logging is OFF by default in this template. Enable to populate PGPT Insights; recommended for local/staging only.
PERAZZI_AI_LOGGING_ENABLED=false

# Text logging truncates stored prompts/responses by default; avoid "full" except short, consented debugging.
PERAZZI_LOG_TEXT_MODE=truncate                # omitted|truncate|full

# Max chars to persist for prompt/response when truncating (keeps default leakage bounded).
PERAZZI_LOG_TEXT_MAX_CHARS=8000              # Max chars to persist for prompt/response
PERAZZI_ASSISTANT_TEMPERATURE=1.0            # Default temperature for Perazzi Assistant route (only when reasoning=none)
PERAZZI_SOUL_JOURNEY_TEMPERATURE=0.6         # Default temperature for Soul Journey route (only when reasoning=none)

# === ZR1 (Archetype Analysis) feature flags ===
# Keep these OFF in production until ZR1 is validated.
PERAZZI_ENABLE_RERANK=false
PERAZZI_RERANK_CANDIDATE_LIMIT=200
PERAZZI_ARCHETYPE_CONFIDENCE_MIN=0.08
PERAZZI_ENABLE_RETRIEVAL_DEBUG=false

# Postgres / pgvector for the knowledge base
DATABASE_URL=postgres://user:password@localhost:5433/perazzi
PGVECTOR_DIM=3072          # match your embedding model's dimension
PGSSL_MODE=disable         # "require" in prod, "disable" for local dev
